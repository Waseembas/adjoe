{
  "version": 4,
  "terraform_version": "1.2.3",
  "serial": 4,
  "lineage": "c40f7db2-b80e-2e11-4184-0b96529a6efd",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "elastic_quickstart",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/default/serviceaccounts/release-name-elasticsearch-client",
            "ignore_fields": null,
            "kind": "ServiceAccount",
            "live_manifest_incluster": "517b19d4e9999f467edf325a3d300312a8988a87072dfffda4a4325acb2c6942",
            "live_uid": "bafc9359-d329-4622-8ea3-eaddb082999e",
            "name": "release-name-elasticsearch-client",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "bafc9359-d329-4622-8ea3-eaddb082999e",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "---\n# Source: elasticsearch/templates/client-serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: \"client\"\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-client\n---\n# Source: elasticsearch/templates/data-serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: \"data\"\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-data\n---\n# Source: elasticsearch/templates/master-serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: \"master\"\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-master\n---\n# Source: elasticsearch/templates/configmap.yaml\napiVersion: v1,/.\n\n;;\nkind: ConfigMap\nmetadata:\n  name: release-name-elasticsearch\n  labels:\n    app: release-name-elasticsearch\n    chart: \"elasticsearch-1.32.5\"\n    release: \"release-name\"\n    heritage: \"Helm\"\ndata:\n  elasticsearch.yml: |-\n    cluster.name: elasticsearch\n\n    node.data: ${NODE_DATA:true}\n    node.master: ${NODE_MASTER:true}\n    node.ingest: ${NODE_INGEST:true}\n    node.name: ${HOSTNAME}\n    network.host: 0.0.0.0\n    # see https://github.com/kubernetes/kubernetes/issues/3595\n    bootstrap.memory_lock: ${BOOTSTRAP_MEMORY_LOCK:false}\n\n    discovery:\n      zen:\n        ping.unicast.hosts: ${DISCOVERY_SERVICE:}\n        minimum_master_nodes: ${MINIMUM_MASTER_NODES:2}\n\n    # see https://github.com/elastic/elasticsearch-definitive-guide/pull/679\n    processors: ${PROCESSORS:}\n\n    # avoid split-brain w/ a minimum consensus of two masters plus a data node\n    gateway.expected_master_nodes: ${EXPECTED_MASTER_NODES:2}\n    gateway.expected_data_nodes: ${EXPECTED_DATA_NODES:1}\n    gateway.recover_after_time: ${RECOVER_AFTER_TIME:5m}\n    gateway.recover_after_master_nodes: ${RECOVER_AFTER_MASTER_NODES:2}\n    gateway.recover_after_data_nodes: ${RECOVER_AFTER_DATA_NODES:1}\n  log4j2.properties: |-\n    status = error\n    appender.console.type = Console\n    appender.console.name = console\n    appender.console.layout.type = PatternLayout\n    appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n\n    rootLogger.level = info\n    rootLogger.appenderRef.console.ref = console\n    logger.searchguard.name = com.floragunn\n    logger.searchguard.level = info\n    \n---\n# Source: elasticsearch/templates/tests/test-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: release-name-elasticsearch-test\n  labels:\n    app: release-name-elasticsearch\n    chart: \"elasticsearch-1.32.5\"\n    heritage: \"Helm\"\n    release: \"release-name\"\ndata:\n  run.sh: |-\n    @test \"Test Access and Health\" {\n      curl -D - http://release-name-elasticsearch-client:9200\n      curl -D - http://release-name-elasticsearch-client:9200/_cluster/health?wait_for_status=green\n    }\n---\n# Source: elasticsearch/templates/client-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: \"client\"\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-client\n\nspec:\n  ports:\n    - name: http\n      port: 9200\n      targetPort: http\n  selector:\n    app: elasticsearch\n    component: \"client\"\n    release: release-name\n  type: NodePort\n   #type: ClusterIP\n---\n# Source: elasticsearch/templates/master-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: \"master\"\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-discovery\nspec:\n  clusterIP: None\n  ports:\n    - port: 9300\n      targetPort: transport\n  selector:\n    app: elasticsearch\n    component: \"master\"\n    release: release-name\n---\n# Source: elasticsearch/templates/client-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: \"client\"\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-client\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch\n      component: \"client\"\n      release: release-name\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n        component: \"client\"\n        release: release-name\n      annotations:\n        checksum/config: 016be46c36e92dde7cf304459972af1dd2a6cd45cd0b1c38b0853fb3702968ff\n    spec:\n      serviceAccountName: release-name-elasticsearch-client\n      securityContext:\n        fsGroup: 1000\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              topologyKey: kubernetes.io/hostname\n              labelSelector:\n                matchLabels:\n                  app: \"elasticsearch\"\n                  release: \"release-name\"\n                  component: \"client\"\n      initContainers:\n      # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n      # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall\n      - name: \"sysctl\"\n        image: \"busybox:latest\"\n        imagePullPolicy: \"Always\"\n        resources:\n            {}\n        command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"]\n        securityContext:\n          privileged: true\n      containers:\n      - name: elasticsearch\n        env:\n        - name: NODE_DATA\n          value: \"false\"\n        - name: NODE_MASTER\n          value: \"false\"\n        - name: DISCOVERY_SERVICE\n          value: release-name-elasticsearch-discovery\n        - name: PROCESSORS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n        - name: ES_JAVA_OPTS\n          value: \"-Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m  \"\n        - name: MINIMUM_MASTER_NODES\n          value: \"2\"\n        resources:\n            limits:\n              cpu: \"1\"\n            requests:\n              cpu: 25m\n              memory: 512Mi\n        readinessProbe:\n          httpGet:\n            path: /_cluster/health\n            port: 9200\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /_cluster/health?local=true\n            port: 9200\n          initialDelaySeconds: 90\n        image: \"elasticsearch:7.17.7\"\n        imagePullPolicy: \"IfNotPresent\"\n        ports:\n        - containerPort: 9200\n          name: http\n        - containerPort: 9300\n          name: transport\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml\n          name: config\n          subPath: elasticsearch.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-elasticsearch\n---\n# Source: elasticsearch/templates/data-statefulset.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: \"data\"\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-data\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch\n      component: \"data\"\n      release: release-name\n      role: data\n  serviceName: release-name-elasticsearch-data\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n        component: \"data\"\n        release: release-name\n        role: data\n    spec:\n      serviceAccountName: release-name-elasticsearch-data\n      securityContext:\n        fsGroup: 1000\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              topologyKey: kubernetes.io/hostname\n              labelSelector:\n                matchLabels:\n                  app: \"elasticsearch\"\n                  release: \"release-name\"\n                  component: \"data\"\n      initContainers:\n      # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n      # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall\n      - name: \"sysctl\"\n        image: \"busybox:latest\"\n        imagePullPolicy: \"Always\"\n        resources:\n            {}\n        command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"]\n        securityContext:\n          privileged: true\n      - name: \"chown\"\n        image: \"elasticsearch:7.17.7\"\n        imagePullPolicy: \"IfNotPresent\"\n        resources:\n            {}\n        command:\n        - /bin/bash\n        - -c\n        - \u003e\n          set -e;\n          set -x;\n          chown elasticsearch:elasticsearch /usr/share/elasticsearch/data;\n          for datadir in $(find /usr/share/elasticsearch/data -mindepth 1 -maxdepth 1 -not -name \".snapshot\"); do\n            chown -R elasticsearch:elasticsearch $datadir;\n          done;\n          chown elasticsearch:elasticsearch /usr/share/elasticsearch/logs;\n          for logfile in $(find /usr/share/elasticsearch/logs -mindepth 1 -maxdepth 1 -not -name \".snapshot\"); do\n            chown -R elasticsearch:elasticsearch $logfile;\n          done\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: data\n      containers:\n      - name: elasticsearch\n        env:\n        - name: DISCOVERY_SERVICE\n          value: release-name-elasticsearch-discovery\n        - name: NODE_MASTER\n          value: \"false\"\n        - name: PROCESSORS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n        - name: ES_JAVA_OPTS\n          value: \"-Djava.net.preferIPv4Stack=true -Xms1536m -Xmx1536m  \"\n        - name: MINIMUM_MASTER_NODES\n          value: \"2\"\n        image: \"elasticsearch:7.17.7\"\n        imagePullPolicy: \"IfNotPresent\"\n        ports:\n        - containerPort: 9300\n          name: transport\n\n        resources:\n            limits:\n              cpu: \"1\"\n            requests:\n              cpu: 25m\n              memory: 1536Mi\n        readinessProbe:\n          httpGet:\n            path: /_cluster/health?local=true\n            port: 9200\n          initialDelaySeconds: 5\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: data\n        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml\n          name: config\n          subPath: elasticsearch.yml\n      terminationGracePeriodSeconds: 3600\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-elasticsearch\n  podManagementPolicy: OrderedReady\n  updateStrategy:\n    type: OnDelete\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n        - \"ReadWriteOnce\"\n      resources:\n        requests:\n          storage: \"30Gi\"\n---\n# Source: elasticsearch/templates/master-statefulset.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: \"master\"\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-master\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch\n      component: \"master\"\n      release: release-name\n      role: master\n  serviceName: release-name-elasticsearch-master\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n        component: \"master\"\n        release: release-name\n        role: master\n    spec:\n      serviceAccountName: release-name-elasticsearch-master\n      securityContext:\n        fsGroup: 1000\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            podAffinityTerm:\n              topologyKey: kubernetes.io/hostname\n              labelSelector:\n                matchLabels:\n                  app: \"elasticsearch\"\n                  release: \"release-name\"\n                  component: \"master\"\n      initContainers:\n      # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n      # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall\n      - name: \"sysctl\"\n        image: \"busybox:latest\"\n        imagePullPolicy: \"Always\"\n        resources:\n            {}\n        command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"]\n        securityContext:\n          privileged: true\n      - name: \"chown\"\n        image: \"elasticsearch:7.17.7\"\n        imagePullPolicy: \"IfNotPresent\"\n        resources:\n            {}\n        command:\n        - /bin/bash\n        - -c\n        - \u003e\n          set -e;\n          set -x;\n          chown elasticsearch:elasticsearch /usr/share/elasticsearch/data;\n          for datadir in $(find /usr/share/elasticsearch/data -mindepth 1 -maxdepth 1 -not -name \".snapshot\"); do\n            chown -R elasticsearch:elasticsearch $datadir;\n          done;\n          chown elasticsearch:elasticsearch /usr/share/elasticsearch/logs;\n          for logfile in $(find /usr/share/elasticsearch/logs -mindepth 1 -maxdepth 1 -not -name \".snapshot\"); do\n            chown -R elasticsearch:elasticsearch $logfile;\n          done\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: data\n      containers:\n      - name: elasticsearch\n        env:\n        - name: NODE_DATA\n          value: \"false\"\n        - name: DISCOVERY_SERVICE\n          value: release-name-elasticsearch-discovery\n        - name: PROCESSORS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n        - name: ES_JAVA_OPTS\n          value: \"-Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m  \"\n        - name: MINIMUM_MASTER_NODES\n          value: \"2\"\n        resources:\n            limits:\n              cpu: \"1\"\n            requests:\n              cpu: 25m\n              memory: 512Mi\n        readinessProbe:\n          httpGet:\n            path: /_cluster/health?local=true\n            port: 9200\n          initialDelaySeconds: 5\n        image: \"elasticsearch:7.17.7\"\n        imagePullPolicy: \"IfNotPresent\"\n        ports:\n        - containerPort: 9300\n          name: transport\n\n        volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: data\n        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml\n          name: config\n          subPath: elasticsearch.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-elasticsearch\n  podManagementPolicy: OrderedReady\n  updateStrategy:\n    type: OnDelete\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n        - \"ReadWriteOnce\"\n      resources:\n        requests:\n          storage: \"4Gi\"\n",
            "yaml_body_parsed": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: elasticsearch\n    chart: elasticsearch-1.32.5\n    component: client\n    heritage: Helm\n    release: release-name\n  name: release-name-elasticsearch-client\n",
            "yaml_incluster": "517b19d4e9999f467edf325a3d300312a8988a87072dfffda4a4325acb2c6942"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "file_beat",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/default/serviceaccounts/filebeat",
            "ignore_fields": null,
            "kind": "ServiceAccount",
            "live_manifest_incluster": "6899dc39193da40431fcd237656fc2bfd7b22c93a56257957ebf4e9bdeb1182b",
            "live_uid": "4d9f6997-47c2-4eea-b6ab-db583af592ba",
            "name": "filebeat",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "4d9f6997-47c2-4eea-b6ab-db583af592ba",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: filebeat\n  labels:\n    k8s-app: filebeat\n\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: filebeat\n  labels:\n    k8s-app: filebeat\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources:\n  - namespaces\n  - pods\n  verbs:\n  - get\n  - watch\n  - list\n  \n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: filebeat\nsubjects:\n- kind: ServiceAccount\n  name: filebeat\nroleRef:\n  kind: ClusterRole\n  name: filebeat\n  apiGroup: rbac.authorization.k8s.io\n\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: filebeat-config\n  labels:\n    k8s-app: filebeat\n    kubernetes.io/cluster-service: \"true\"\ndata:\n  filebeat.yml: |-\n    filebeat.config:\n      prospectors:\n        # Mounted `filebeat-prospectors` configmap:\n        path: ${path.config}/prospectors.d/*.yml\n        # Reload prospectors configs as they change:\n        reload.enabled: false\n      modules:\n        path: ${path.config}/modules.d/*.yml\n        # Reload module configs as they change:\n        reload.enabled: false\n    processors:\n      - add_cloud_metadata:\n\n    cloud.id: ${ELASTIC_CLOUD_ID}\n    cloud.auth: ${ELASTIC_CLOUD_AUTH}\n\n    output.logstash:\n      hosts: ['logstash-service:5044']\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: filebeat-prospectors\n  labels:\n    k8s-app: filebeat\n    kubernetes.io/cluster-service: \"true\"\ndata:\n  kubernetes.yml: |-\n    - type: docker\n      containers.ids:\n      - \"*\"\n      processors:\n        - add_kubernetes_metadata:\n            in_cluster: true\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: filebeat\n  labels:\n    k8s-app: filebeat\n    kubernetes.io/cluster-service: \"true\"\nspec:\n  selector:\n    matchLabels:\n      k8s-app: filebeat\n  template:\n    metadata:\n      labels:\n        k8s-app: filebeat\n        kubernetes.io/cluster-service: \"true\"\n    spec:\n      serviceAccountName: filebeat\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: filebeat\n        image: docker.elastic.co/beats/filebeat:6.3.0\n        args: [\n          \"-c\", \"/etc/filebeat.yml\",\n          \"-e\",\n        ]\n        env:\n        - name: ELASTICSEARCH_HOST\n          value: elasticsearch-logging\n        - name: ELASTICSEARCH_PORT\n          value: \"9200\"\n        - name: ELASTICSEARCH_USERNAME\n          value: elastic\n        - name: ELASTICSEARCH_PASSWORD\n          value: changeme\n        - name: ELASTIC_CLOUD_ID\n          value:\n        - name: ELASTIC_CLOUD_AUTH\n          value:\n        securityContext:\n          runAsUser: 0\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/filebeat.yml\n          readOnly: true\n          subPath: filebeat.yml\n        - name: prospectors\n          mountPath: /usr/share/filebeat/prospectors.d\n          readOnly: true\n        - name: data\n          mountPath: /usr/share/filebeat/data\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          defaultMode: 0600\n          name: filebeat-config\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: prospectors\n        configMap:\n          defaultMode: 0600\n          name: filebeat-prospectors\n      - name: data\n        emptyDir: {}\n",
            "yaml_body_parsed": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: filebeat\n  name: filebeat\n",
            "yaml_incluster": "6899dc39193da40431fcd237656fc2bfd7b22c93a56257957ebf4e9bdeb1182b"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubectl_manifest.elastic_quickstart",
            "kubectl_manifest.kibana_quickstart"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "kibana_quickstart",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "apps/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/apps/v1/namespaces/kube-system/deployments/kibana-logging",
            "ignore_fields": null,
            "kind": "Deployment",
            "live_manifest_incluster": "6880748bb18755a826e31d78a82e0c80b5dadbc130c57b42e5004e5afee7a5ba",
            "live_uid": "da6c224d-d640-4d40-8c3f-01fa8bef36bc",
            "name": "kibana-logging",
            "namespace": "kube-system",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "da6c224d-d640-4d40-8c3f-01fa8bef36bc",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kibana-logging\n  namespace: kube-system\n  labels:\n    k8s-app: kibana-logging\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kibana-logging\n  template:\n    metadata:\n      labels:\n        k8s-app: kibana-logging\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'\n    spec:\n      containers:\n      - name: kibana-logging\n        image: docker.elastic.co/kibana/kibana-oss:6.2.4\n        resources:\n          # need more cpu upon initialization, therefore burstable class\n          limits:\n            cpu: 1000m\n          requests:\n            cpu: 100m\n        env:\n          - name: ELASTICSEARCH_URL\n            value: http://elasticsearch-logging:9200\n        ports:\n        - containerPort: 5601\n          name: ui\n          protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kibana-logging\n  namespace: kube-system\n  labels:\n    k8s-app: kibana-logging\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"Kibana\"\nspec:\n  type: NodePort\n  ports:\n  - port: 5601\n    protocol: TCP\n    targetPort: ui\n    nodePort: 31336\n  selector:\n    k8s-app: kibana-logging\n# ---\n# apiVersion: extensions/v1beta1\n# kind: Ingress\n# metadata:\n#   name: logs-ingress\n#   namespace: kube-system\n#   labels:\n#     app: logs\n#   annotations:\n#     kubernetes.io/ingress.class: traefik\n# spec:\n#   rules:\n#   - host: logs.example.com\n#     http:\n#       paths:\n#       - path: /\n#         backend:\n#           serviceName: kibana-logging\n#           servicePort: 5601\n",
            "yaml_body_parsed": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    k8s-app: kibana-logging\n    kubernetes.io/cluster-service: \"true\"\n  name: kibana-logging\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kibana-logging\n  template:\n    metadata:\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: docker/default\n      labels:\n        k8s-app: kibana-logging\n    spec:\n      containers:\n      - env:\n        - name: ELASTICSEARCH_URL\n          value: http://elasticsearch-logging:9200\n        image: docker.elastic.co/kibana/kibana-oss:6.2.4\n        name: kibana-logging\n        ports:\n        - containerPort: 5601\n          name: ui\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 1000m\n          requests:\n            cpu: 100m\n",
            "yaml_incluster": "6880748bb18755a826e31d78a82e0c80b5dadbc130c57b42e5004e5afee7a5ba"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubectl_manifest.elastic_quickstart"
          ]
        }
      ]
    }
  ]
}
